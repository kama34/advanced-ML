{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Lab 7 :  Graphical and hidden Markov models\n",
    "```\n",
    "- [S23] Advanced Machine Learning, Innopolis University\n",
    "- Teaching Assistant: Gcinizwe Dlamini\n",
    "```\n",
    "<hr>\n",
    "\n",
    "\n",
    "```\n",
    "Lab Plan\n",
    "1. Project start\n",
    "2. Recap on HMMs\n",
    "3. Simple example for HMMs\n",
    "3. Task\n",
    "```\n",
    "\n",
    "<hr>\n"
   ],
   "metadata": {
    "id": "9tDI0HSwkegm"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1. Recap on HMMs and Likelihood\n",
    "\n",
    "<!--![](https://www.dropbox.com/scl/fi/wm8nbmqgchv3orw7r488f/Lecture-5-HMM.jpg?rlkey=3x0lmmzxchgz6esmq6yrjfzne&dl=1)-->"
   ],
   "metadata": {
    "id": "ZnuGqnsqmxlQ"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "\n",
    "states = ('Sunny', 'Rainy')\n",
    "observations = ('happy', 'grumpy')\n",
    "pi = np.array([2. / 3., 1. / 3.])  # initial probability\n",
    "A = np.array([[7. / 9., 2. / 9.], [0.4, 0.6]])  # Transmission probability\n",
    "B = np.array([[0.8, 0.2], [0.4, 0.6]])  # Emission probability\n",
    "bob_says = np.array([0, 0, 1, 1, 1, 0])\n",
    "\n",
    "\n",
    "def forward(obs_seq, pi, A, B):\n",
    "    T = len(obs_seq)\n",
    "    N = A.shape[0]\n",
    "    alpha = np.zeros((T, N))\n",
    "    alpha[0] = pi * B[:, obs_seq[0]]\n",
    "    for t in range(1, T):\n",
    "        alpha[t] = np.inner(alpha[t - 1], A) * B[:, obs_seq[t]]\n",
    "    return alpha\n",
    "\n",
    "\n",
    "def likelihood(alpha):\n",
    "    # using the forward part of the forward-backward algorithm\n",
    "    return alpha[-1].sum()\n",
    "\n",
    "\n",
    "alpha = forward(bob_says, pi, A, B)\n",
    "print(alpha)\n",
    "\n",
    "print(likelihood(alpha))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wCjG1przOcMS",
    "outputId": "b381db53-d799-4bac-b60a-18993cd742db",
    "ExecuteTime": {
     "end_time": "2024-03-18T23:51:45.868941Z",
     "start_time": "2024-03-18T23:51:45.670051Z"
    }
   },
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.53333333 0.13333333]\n",
      " [0.35555556 0.11733333]\n",
      " [0.06052346 0.12757333]\n",
      " [0.01508469 0.06045203]\n",
      " [0.00503326 0.02538306]\n",
      " [0.00764435 0.00689726]]\n",
      "0.014541607035957256\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. Viterbi algorithm simple example\n"
   ],
   "metadata": {
    "id": "5M67p6tEOjm3"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from numpy import random\n",
    "\n",
    "# Transition Probabilities (s - sunny; r-raining)\n",
    "p_ss = 7. / 9.\n",
    "p_sr = 2. / 9.\n",
    "p_rs = 0.4\n",
    "p_rr = 0.6\n",
    "\n",
    "# Initial Probabilities\n",
    "p_s = 2 / 3\n",
    "p_r = 1 / 3\n",
    "\n",
    "# Emission Probabilities (h - happy; g- grumpy)\n",
    "p_sh = 0.8\n",
    "p_sg = 0.2\n",
    "p_rh = 0.4\n",
    "p_rg = 0.6\n",
    "\n",
    "moods = ['H', 'H', 'G', 'G', 'G', 'H']\n",
    "probabilities = []\n",
    "weather = []\n",
    "\n",
    "if moods[0] == 'H':\n",
    "    probabilities.append((p_s * p_sh, p_r * p_rh))\n",
    "else:\n",
    "    probabilities.append((p_s * p_sg, p_r * p_rg))\n",
    "\n",
    "for i in range(1, len(moods)):\n",
    "    yesterday_sunny, yesterday_rainy = probabilities[-1]\n",
    "    if moods[i] == 'H':\n",
    "        today_sunny = max(yesterday_sunny * p_ss * p_sh, yesterday_rainy * p_rs * p_sh)\n",
    "        today_rainy = max(yesterday_sunny * p_sr * p_rh, yesterday_rainy * p_rr * p_rh)\n",
    "        probabilities.append((today_sunny, today_rainy))\n",
    "    else:\n",
    "        today_sunny = max(yesterday_sunny * p_ss * p_sg, yesterday_rainy * p_rs * p_sg)\n",
    "        today_rainy = max(yesterday_sunny * p_sr * p_rg, yesterday_rainy * p_rr * p_rg)\n",
    "        probabilities.append((today_sunny, today_rainy))\n",
    "\n",
    "for p in probabilities:\n",
    "    if p[0] > p[1]:\n",
    "        weather.append('S')\n",
    "    else:\n",
    "        weather.append('R')\n",
    "\n",
    "weather"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HsUXvGSGPKjF",
    "outputId": "752f55d6-af53-4b4e-dd69-a99be822cc99",
    "ExecuteTime": {
     "end_time": "2024-03-18T23:51:49.710157Z",
     "start_time": "2024-03-18T23:51:49.691737Z"
    }
   },
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "['S', 'S', 'S', 'R', 'R', 'S']"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3. Parts of Speech (POS) Tagging\n",
    "\n",
    "---\n",
    "\n",
    "* process of assigning a part-of-speech to each word in a text\n",
    "* POS is a disambiguation task (some words can have multiple part of speech depending in the context)\n",
    "* Gives an idea about syntactic structure (parsing)\n",
    "\n",
    "**How does it work?**\n",
    "\n",
    "<!--![](http://www.cs.virginia.edu/~hw5x/Course/TextMining-2019Spring/_site/docs/codes/HMM.PNG)-->"
   ],
   "metadata": {
    "id": "muaU84GNpgY9"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4. Task (Viterbi algorithm)\n",
    "\n",
    "<!-- ![](https://miro.medium.com/max/1080/1*8-5KZVj-_jZOWN83gGhD5A.png) -->\n",
    "<!--![](https://www.dropbox.com/scl/fi/twg8yqdhqwxcbr5lcqj4w/pos_tag1.png?rlkey=5kizqwfv8gk8vg26434sp0rk2&dl=1)-->\n",
    "\n",
    "First steps :\n",
    "*  Calculate or estimate transition probabilities between different parts of speech tags\n",
    "* Calculate or estimate prior probabilities of tags\n",
    "\n",
    "\n",
    "In the task you are going to use a a fraction of conll2000 dataset. The dataset should be retrieved from : [Train](https://www.dropbox.com/s/x9n6f9o9jl7pno8/train_pos.txt?dl=1), [Test](https://www.dropbox.com/s/v8nccvq7jewcl8s/test_pos.txt?dl=1)\n",
    "\n",
    "1. Calculate the transition probability and emission matrices (First step towards viterbi) <!-- 10 points -->\n",
    "1. Use Viterbi algorithm for POS tagging task (you can use libiries such as [`hmmlearn`](https://github.com/hmmlearn/hmmlearn)).\n",
    "1. Test your viterbi model on the test set and record the accuracy. The accuray referes to the number of correcly predicted tags in the whole test samples.\n",
    "1. Using Recurrent neural network (RNN, GRU or LSTM) or Transfomer solve the task for POS"
   ],
   "metadata": {
    "id": "Jg_g0rs_dqfl"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Load dataset train and test\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset('conll2000')\n",
    "\n",
    "train_data = dataset['train']\n",
    "test_data = dataset['test']\n",
    "\n",
    "print(len(train_data))"
   ],
   "metadata": {
    "id": "4OMiWRjLoWKP"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19, 14, 11, 19, 39, 27, 37, 32, 34, 11, 15, 19, 14, 19, 22, 14, 20, 5, 15, 14, 19, 19, 5, 34, 32, 34, 11, 15, 19, 14, 20, 9, 20, 24, 15, 22, 6]\n"
     ]
    }
   ],
   "source": [
    "print(train_data[0]['pos_tags'])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-19T00:27:40.893205Z",
     "start_time": "2024-03-19T00:27:40.889203Z"
    }
   },
   "execution_count": 13
  },
  {
   "cell_type": "markdown",
   "source": [
    "    - name: pos_tags\n",
    "      sequence:\n",
    "        class_label:\n",
    "          names:\n",
    "            '0': ''''''\n",
    "            '1': '#'\n",
    "            '2': $\n",
    "            '3': (\n",
    "            '4': )\n",
    "            '5': ','\n",
    "            '6': .\n",
    "            '7': ':'\n",
    "            '8': '``'\n",
    "            '9': CC\n",
    "            '10': CD\n",
    "            '11': DT\n",
    "            '12': EX\n",
    "            '13': FW\n",
    "            '14': IN\n",
    "            '15': JJ\n",
    "            '16': JJR\n",
    "            '17': JJS\n",
    "            '18': MD\n",
    "            '19': NN\n",
    "            '20': NNP\n",
    "            '21': NNPS\n",
    "            '22': NNS\n",
    "            '23': PDT\n",
    "            '24': POS\n",
    "            '25': PRP\n",
    "            '26': PRP$\n",
    "            '27': RB\n",
    "            '28': RBR\n",
    "            '29': RBS\n",
    "            '30': RP\n",
    "            '31': SYM\n",
    "            '32': TO\n",
    "            '33': UH\n",
    "            '34': VB\n",
    "            '35': VBD\n",
    "            '36': VBG\n",
    "            '37': VBN\n",
    "            '38': VBP\n",
    "            '39': VBZ\n",
    "            '40': WDT\n",
    "            '41': WP\n",
    "            '42': WP$\n",
    "            '43': WRB\n",
    "    - name: chunk_tags\n",
    "      sequence:\n",
    "        class_label:\n",
    "          names:\n",
    "            '0': O\n",
    "            '1': B-ADJP\n",
    "            '2': I-ADJP\n",
    "            '3': B-ADVP\n",
    "            '4': I-ADVP\n",
    "            '5': B-CONJP\n",
    "            '6': I-CONJP\n",
    "            '7': B-INTJ\n",
    "            '8': I-INTJ\n",
    "            '9': B-LST\n",
    "            '10': I-LST\n",
    "            '11': B-NP\n",
    "            '12': I-NP\n",
    "            '13': B-PP\n",
    "            '14': I-PP\n",
    "            '15': B-PRT\n",
    "            '16': I-PRT\n",
    "            '17': B-SBAR\n",
    "            '18': I-SBAR\n",
    "            '19': B-UCP\n",
    "            '20': I-UCP\n",
    "            '21': B-VP\n",
    "            '22': I-VP"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8937\n"
     ]
    }
   ],
   "source": [
    "print(train_data.shape[0])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-19T01:37:27.250929Z",
     "start_time": "2024-03-19T01:37:27.247348Z"
    }
   },
   "execution_count": 79
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# Calculate transition probabilities between different part of speach tags\n",
    "def calculate_transition_probability(data):\n",
    "    # Dictionary for count tags\n",
    "    tag_counts = defaultdict(int)\n",
    "    # Dictionary for count transition between every POS\n",
    "    transition_counts = defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "    for features in data:\n",
    "        # Length sentence\n",
    "        size = len(features['pos_tags'])\n",
    "\n",
    "        for index, pos_tag in enumerate(features['pos_tags']):\n",
    "            if index < size - 1:\n",
    "                next_pos_tag = features['pos_tags'][index + 1]\n",
    "\n",
    "                # Calculate count tags in sentences\n",
    "                tag_counts[pos_tag] += 1\n",
    "\n",
    "                # Calculate count transition between POS_t and POS_{t+1}\n",
    "                transition_counts[pos_tag][next_pos_tag] += 1\n",
    "\n",
    "    # Dictionary for probability transition\n",
    "    transition_probability = defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "    # Calculate transition probabilities for every transition pos\n",
    "    for tag in transition_counts:\n",
    "        for next_tag in transition_counts[tag]:\n",
    "            transition_probability[tag] = transition_counts[tag][next_tag] / tag_counts[next_tag]\n",
    "    return transition_probability"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-19T01:22:04.928208Z",
     "start_time": "2024-03-19T01:22:04.924290Z"
    }
   },
   "execution_count": 45
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "defaultdict(<function __main__.calculate_transition_probability.<locals>.<lambda>()>,\n            {19: 0.024096385542168676,\n             14: 0.004016064257028112,\n             11: 0.26666666666666666,\n             39: 0.0036496350364963502,\n             27: 0.02631578947368421,\n             37: 0.0024096385542168677,\n             32: 0.003115264797507788,\n             34: 0.00267379679144385,\n             15: 0.01818181818181818,\n             22: 0.0053475935828877,\n             20: 0.001594896331738437,\n             5: 0.001049317943336831,\n             9: 0.012048192771084338,\n             24: 0.004016064257028112,\n             38: 0.0024096385542168677,\n             36: 0.0003486750348675035,\n             26: 0.00018615040953090097,\n             10: 0.000531632110579479,\n             8: 0.003115264797507788,\n             0: 0.0005714285714285715,\n             35: 0.0013844023996308261,\n             12: 0.00019688915140775743,\n             18: 0.004016064257028112,\n             1: 0.004331608711346409,\n             3: 0.0003056234718826406,\n             2: 5.034739703957305e-05,\n             4: 0.0002406449284081338,\n             21: 0.000975609756097561,\n             25: 0.005235602094240838,\n             17: 0.0024096385542168677,\n             41: 0.0001528584530724549,\n             28: 0.0005652911249293386,\n             16: 0.0024096385542168677,\n             6: 6.637241562406664e-05,\n             40: 0.0005652911249293386,\n             43: 0.00018573551263001485,\n             29: 9.286775631500742e-05,\n             23: 0.003000054546446299,\n             30: 0.0001203224642040669,\n             7: 0.002926829268292683,\n             13: 0.00018615040953090097,\n             42: 0.0011737089201877935,\n             31: 0.05263157894736842,\n             33: 4.39386616283668e-05})"
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_transition_probability(train_data)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-19T01:22:06.472494Z",
     "start_time": "2024-03-19T01:22:05.672087Z"
    }
   },
   "execution_count": 46
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Calculate prior probabilities\n",
    "def calculate_prior_probabilities(data):\n",
    "    # Dictionary count every POS start in sentence\n",
    "    start_tag_counts = defaultdict(int)\n",
    "s\n",
    "    # Count total sentences\n",
    "    total_sentences = 0\n",
    "    \n",
    "    for features in data:\n",
    "        if features['pos_tags']:\n",
    "            start_tag = features['pos_tags'][0]\n",
    "            start_tag_counts[start_tag] += 1\n",
    "            total_sentences += 1\n",
    "    \n",
    "    # Dictionary prior probabilities for every POS\n",
    "    prior_probability = defaultdict(int)\n",
    "    \n",
    "    for tag in start_tag_counts:\n",
    "        prior_probability[tag] = start_tag_counts[tag] / total_sentences\n",
    "        \n",
    "    return prior_probability"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-19T01:43:41.789623Z",
     "start_time": "2024-03-19T01:43:41.785796Z"
    }
   },
   "execution_count": 89
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "defaultdict(int,\n            {19: 0.045210384959713516,\n             20: 0.19192032229185318,\n             9: 0.056177260519247985,\n             11: 0.21239928379588183,\n             8: 0.07273948075201432,\n             22: 0.04364368845120859,\n             14: 0.12891674127126232,\n             25: 0.0583034914950761,\n             27: 0.06031781557743957,\n             37: 0.005259623992837959,\n             15: 0.044874664279319604,\n             26: 0.00861683079677708,\n             43: 0.006378692927484333,\n             34: 0.004811996418979409,\n             41: 0.0032452999104744854,\n             10: 0.016226499552372427,\n             17: 0.004923903312444047,\n             32: 0.0033572068039391225,\n             36: 0.01342882721575649,\n             3: 0.003133393017009848,\n             16: 0.0011190689346463742,\n             0: 0.0004476275738585497,\n             28: 0.002126230975828111,\n             35: 0.0011190689346463742,\n             12: 0.0032452999104744854,\n             39: 0.0019024171888988362,\n             31: 0.0004476275738585497,\n             38: 0.0003357206803939123,\n             40: 0.000783348254252462,\n             33: 0.0006714413607878246,\n             7: 0.002126230975828111,\n             18: 0.0005595344673231871,\n             4: 0.00011190689346463742,\n             21: 0.00022381378692927484,\n             42: 0.00011190689346463742,\n             2: 0.0006714413607878246,\n             1: 0.00011190689346463742})"
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_prior_probabilities(train_data)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-19T01:43:55.192259Z",
     "start_time": "2024-03-19T01:43:54.483765Z"
    }
   },
   "execution_count": 91
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ]
}
